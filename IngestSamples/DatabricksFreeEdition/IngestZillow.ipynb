{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27154031-c12f-4bb6-bdb1-592e2fef8f57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Ingest Zillow\n",
    "Demonstrates accessing data stored as a file at a specific URL.  The data can be browsed at https://www.zillow.com/research/data/.  The specific URLs can be copied with a right click on the Download button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59e17629-55c4-4a9b-9e41-2db71c83da99",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1758199740431}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "\n",
    "# This URL returns a monthly time series of number of sales in the US metro regions.  \n",
    "# The URL can be determiend by \"Copy link\" when right clicking the Download button after setting up your desired Data Type and Geography.\n",
    "url = \"https://files.zillowstatic.com/research/public_csvs/sales_count_now/Metro_sales_count_now_uc_sfrcondo_month.csv?t=1757774212\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# The function raise_for_status throws an exception if the response was not successful.  \n",
    "response.raise_for_status()\n",
    "\n",
    "# BytesIO is a file-like object that can be used to read the content of the response.  This enables the data to be read into a Pandas dataframe. \n",
    "file_content = BytesIO(response.content)\n",
    "sales_count_pdf = pd.read_csv(file_content)\n",
    "display(sales_count_pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82528292-7619-4968-9444-cd2fc12331f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert to a PySpark dataframe.\n",
    "sales_count_df = spark.createDataFrame(sales_count_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbe9e842-eb0c-4613-8290-f6c0dc13ac1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Compute null counts for all columns in one pass\n",
    "null_counts = sales_count_df.select([\n",
    "    sum(col(c).isNull().cast(\"int\")).alias(c) for c in sales_count_df.columns\n",
    "        ]).collect()[0].asDict()\n",
    "\n",
    "# Get columns where all values are null\n",
    "row_count = sales_count_df.count()\n",
    "null_columns = [c for c, n in null_counts.items() if n == row_count]\n",
    "\n",
    "if len(null_columns) != 0:\n",
    "    display(pd.DataFrame({\"all_null_columns\": null_columns}))\n",
    "    sales_count_df = sales_count_df.drop(*null_columns)\n",
    "\n",
    "display(sales_count_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfe69df6-c95a-4976-be05-c79b461ae7aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create the destination schema if needed.  The 'bronze' schema aligns to the raw ingestion tier of the medallion architecture.\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS bronze_examples\")\n",
    "\n",
    "# Save in the bronze_examples schema.\n",
    "sales_count_df.write.mode(\"overwrite\").saveAsTable(\"bronze_examples.sales_count\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "IngestZillow",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
