{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c4d42aa-8337-4196-ab85-473f2409113d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install the pybaseball package on the server that is running this notebook.\n",
    "%pip install pybaseball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "203234b8-b163-4012-a6f3-42b5d8ede9a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pybaseball import pitching_stats\n",
    "import pandas as pd\n",
    "\n",
    "pitching_pdf = pitching_stats(2022,2024)\n",
    "\n",
    "display(pitching_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36a17f68-d69f-43af-9dc3-b631d55814c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Apply column name conversions to comply with catalog limitations\n",
    "def clean_column_name(col_name):\n",
    "    col_name = col_name.replace('%', '_pct')\n",
    "    col_name = col_name.replace('(', '_').replace(')', '_')\n",
    "    col_name = col_name.replace(' ', '')\n",
    "    col_name = re.sub('_+', '_', col_name)  # Replace multiple underscores with single\n",
    "    col_name = col_name.strip('_')  # Remove leading/trailing underscores\n",
    "    return col_name\n",
    "\n",
    "new_columns = [clean_column_name(col) for col in pitching_pdf.columns]\n",
    "pitching_pdf.columns = new_columns\n",
    "\n",
    "display(pitching_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01cc49cd-b95d-4a72-a6be-15cbda6f85e1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1758200692451}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Confirm that this is a Pandas dataframe by displaying the type.  Convert to a PySpark dataframe before saving.\n",
    "print(type(pitching_pdf))\n",
    "\n",
    "pitching_df = spark.createDataFrame(pitching_pdf)\n",
    "display(pitching_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3421b352-8d9b-4fd3-be00-b91566f625bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Delete columns that are all null.  Slow - it scanes the entire dataframe for each column individually. Using a for loop is usually a bad idea in PySpark.\n",
    "null_columns = [c for c in pitching_df.columns if pitching_df.filter(pitching_df[c].isNotNull()).count() == 0]\n",
    "display(pd.DataFrame({\"all_null_columns\": null_columns}))\n",
    "\n",
    "pitching_sub_df = pitching_df.drop(*null_columns)\n",
    "display(pitching_sub_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c884933f-d016-472a-affb-13220fa5cba6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Delete columns that are all null... fast - aggregate functions speed things up.\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Compute null counts for all columns in one pass\n",
    "null_counts = pitching_df.select([\n",
    "    sum(col(c).isNull().cast(\"int\")).alias(c) for c in pitching_df.columns\n",
    "        ]).collect()[0].asDict()\n",
    "\n",
    "# Get columns where all values are null\n",
    "row_count = pitching_df.count()\n",
    "null_columns = [c for c, n in null_counts.items() if n == row_count]\n",
    "print(null_columns)\n",
    "\n",
    "if len(null_columns) != 0:\n",
    "    display(pd.DataFrame({\"all_null_columns\": null_columns}))\n",
    "    pitching_df = pitching_df.drop(*null_columns)\n",
    "\n",
    "display(pitching_sub_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da819153-698c-4cd2-bc5e-08f383c5824a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the destination schema if needed.  The 'bronze' schema aligns to the raw ingestion tier of the medallion architecture.\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS bronze_examples\")\n",
    "\n",
    "# Catalogs have limitations on column names.  Do the following conversions:\n",
    "#   - Convert % to _pct\n",
    "#   - Convert ( and ) to _\n",
    "#   - Remove spaces\n",
    "\n",
    "\n",
    "# Store pitching_df in a table in the bronze schema\n",
    "pitching_df.write.mode(\"overwrite\").saveAsTable(\"bronze_examples.pitching\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "IngestBaseball",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
