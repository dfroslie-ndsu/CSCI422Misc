{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26d9e66d-6ca2-4738-a75d-995ae292a186",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Ingest Kaggle\n",
    "Example of ingesting from Kaggle.com.  This ingestion has a couple of unique factors:\n",
    "- It uses a client library that provides an easier API to use the REST calls.\n",
    "- It typically provides the data in zipped CSV format.  We want to minimally unzip before saving.\n",
    "\n",
    "For documentation on the Kaggle API, go to https://www.kaggle.com/docs/api.  This page includes the following important authentication information.  You will need the downloaded file and will use it below.\n",
    "\n",
    "\"In order to use the Kaggleâ€™s public API, you must first authenticate using an API token. Go to the 'Account' tab of your user profile and select 'Create New Token'. This will trigger the download of kaggle.json, a file containing your API credentials.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0430d915-ce93-4cec-9bde-c3a334681772",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# I would typically put this block into a shared utility to avoid the code duplication, but leaving \n",
    "# it here to be explicit for demonstration.\n",
    "\n",
    "# Assign variables to clarify inputs to the spark.conf.set() call.\n",
    "my_scope = \"Fall2025SecretScope\"   # Databricks secret scope.\n",
    "my_key = \"assign1store\"             # Key vaault secret containing storage account access key.\n",
    "storage_end_point = \"assign1store.dfs.core.windows.net\"  # Storage account uri.\n",
    "container_name = \"misc\"    # Container name.\n",
    "\n",
    "# The following spark configuration call uses the variables set above.\n",
    "spark.conf.set(\n",
    "    \"fs.azure.account.key.\" + storage_end_point,\n",
    "    dbutils.secrets.get(scope=my_scope, key=my_key))\n",
    "\n",
    "# To set the URI to be used in the code below, the container name (assign-1-blob) in the string.\n",
    "uri = \"abfss://\" + container_name + \"@\" + storage_end_point + \"/\" \n",
    "print(uri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c6027d6-fadc-4a6c-b9aa-19bebfd1f274",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The Kaggle library won't be installed on the Databricks cluster by default.  So we need to \n",
    "# install it here.\n",
    "%pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8a2c45a-4271-4344-884e-2325b5f79a41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# When your register with Kaggle, a JSON file containing your account key is provided.  \n",
    "# Kaggle expects this to be in a well known location in the file system.\n",
    "# On a PC, this is in c:\\users\\<YourUserName>\\.kaggle\\kaggle.json\n",
    "# For Databricks, we'll put it in the following location.\n",
    "\n",
    "# Create the directory for Kaggle JSON\n",
    "dbutils.fs.mkdirs(\"file:/root/.kaggle\")\n",
    "\n",
    "# Copy the Kaggle JSON file from Azure storage to the expected location\n",
    "source_location = uri + \"AccessInfo/kaggle.json\"\n",
    "dbutils.fs.cp(source_location, \"file:/root/.kaggle/kaggle.json\")\n",
    "\n",
    "# Set the access to kaggle.json to 600 (owner: read/write, group: none, other: none)\n",
    "!chmod 600 /root/.kaggle/kaggle.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f81e26e6-2e3f-40c0-ae22-e18ba329420b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Authentication defaults to use the config file in the predefined location.\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Get the competitions list to test out the API.\n",
    "competitions = api.competitions_list()\n",
    "print(competitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fae0f756-6c3c-40de-94c4-fc482c312154",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define dataset and output path\n",
    "dataset = 'hesh97/titanicdataset-traincsv'\n",
    "out_path_temp = '/dbfs/mnt/assign1store/misc/Bronze/Kaggle/Zip'  # Temporary location; can't write direct to ADLS.\n",
    "out_path_ADLS = uri + 'Bronze/Kaggle/Titanic'  # Azure storage\n",
    "\n",
    "# Download dataset files to the specified path, unzipping the files.\n",
    "api.dataset_download_files(dataset, path=out_path_temp, unzip=True)\n",
    "\n",
    "# Copy the downloaded files to Azure storage\n",
    "dbutils.fs.cp(\"file:\" + out_path_temp, out_path_ADLS, recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b5ba7ca-69f2-4d2d-a522-4274e44dbf61",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1756848645674}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read the file and display.\n",
    "df = spark.read.csv(out_path_ADLS, header=True, inferSchema=True)\n",
    "display(df)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "IngestKaggle",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
